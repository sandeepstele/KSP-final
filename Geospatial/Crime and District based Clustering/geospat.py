# -*- coding: utf-8 -*-
"""geospat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7mb_lfy9sQfJP5jiYD4fWcPueSnZIKV
"""

import pandas as pd
import random

# Load the FIR_Details_Data.csv file into a DataFrame
fir_data = pd.read_csv('FIR_Details_Data.csv')

# Define the bounding box for Karnataka
karnataka_bbox = {
    'min_lat': 11.5934,
    'max_lat': 18.1667,
    'min_lon': 74.0500,
    'max_lon': 78.5885
}

# Filter out locations outside Karnataka
karnataka_data = fir_data[
    (fir_data['Latitude'] >= karnataka_bbox['min_lat']) &
    (fir_data['Latitude'] <= karnataka_bbox['max_lat']) &
    (fir_data['Longitude'] >= karnataka_bbox['min_lon']) &
    (fir_data['Longitude'] <= karnataka_bbox['max_lon'])
]

# Randomly select 40,000 entries after filtering
selected_data = karnataka_data.sample(n=80000, random_state=42)

# Write the selected data to a new CSV file
selected_data.to_csv('selected_fir_data.csv', index=False)

print("New CSV file with 80,000 rows from FIR data in Karnataka has been created.")

# Drop rows with missing latitude or longitude values
fir_data.dropna(subset=['Latitude', 'Longitude'], inplace=True)

# Convert date columns to datetime format
fir_data['Offence_From_Date'] = pd.to_datetime(fir_data['Offence_From_Date'])
fir_data['Offence_To_Date'] = pd.to_datetime(fir_data['Offence_To_Date'])
fir_data['FIR_Reg_DateTime'] = pd.to_datetime(fir_data['FIR_Reg_DateTime'])
fir_data['FIR_Date'] = pd.to_datetime(fir_data['FIR_Date'])

import matplotlib.pyplot as plt

# Histogram of crime types
plt.figure(figsize=(10, 6))
fir_data['CrimeGroup_Name'].value_counts().plot(kind='bar')
plt.title('Distribution of Crime Types')
plt.xlabel('Crime Type')
plt.ylabel('Frequency')
plt.show()

# Use spatial analysis techniques such as clustering or KDE to analyze spatial patterns
# Example: Apply KDE to estimate crime density
from scipy.stats import gaussian_kde

kde = gaussian_kde(fir_data[['Latitude', 'Longitude']].T)
density = kde(fir_data[['Latitude', 'Longitude']].T)

# Visualize crime density using a heatmap
plt.figure(figsize=(10, 8))
plt.scatter(fir_data['Longitude'], fir_data['Latitude'], c=density, cmap='Reds', s=5)
plt.title('Crime Density Heatmap')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.colorbar(label='Density')
plt.show()

# Extract year and month from datetime columns for temporal analysis
fir_data['Year'] = fir_data['Offence_From_Date'].dt.year
fir_data['Month'] = fir_data['Offence_From_Date'].dt.month

# Visualize temporal patterns using line plots or histograms
plt.figure(figsize=(10, 6))
fir_data.groupby(['Year', 'Month']).size().plot()
plt.title('Crime Distribution Over Time')
plt.xlabel('Time')
plt.ylabel('Frequency')
plt.show()

# Apply hotspot analysis techniques such as clustering or KDE
# Example: Use DBSCAN for cluster identification
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.1, min_samples=10)
clusters = dbscan.fit_predict(fir_data[['Latitude', 'Longitude']])

# Visualize crime hotspots based on clusters
plt.figure(figsize=(10, 8))
plt.scatter(fir_data['Longitude'], fir_data['Latitude'], c=clusters, cmap='viridis', s=5)
plt.title('Crime Hotspots Identified by DBSCAN')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.colorbar(label='Cluster')
plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Load the FIR_Details_Data.csv file into a GeoDataFrame
#fir_data = gpd.read_file('FIR_Details_Data.csv')

# Convert latitude and longitude columns to numeric data types
fir_data['Latitude'] = pd.to_numeric(fir_data['Latitude'], errors='coerce')
fir_data['Longitude'] = pd.to_numeric(fir_data['Longitude'], errors='coerce')

# Filter out rows with missing or invalid latitude and longitude values
fir_data = fir_data.dropna(subset=['Latitude', 'Longitude'])

# Define the bounding box for Karnataka
karnataka_bbox = {
    'min_lat': 11.5934,
    'max_lat': 18.1667,
    'min_lon': 74.0500,
    'max_lon': 78.5885
}

# Filter out locations outside Karnataka
karnataka_data = fir_data[
    (fir_data['Latitude'] >= karnataka_bbox['min_lat']) &
    (fir_data['Latitude'] <= karnataka_bbox['max_lat']) &
    (fir_data['Longitude'] >= karnataka_bbox['min_lon']) &
    (fir_data['Longitude'] <= karnataka_bbox['max_lon'])
]

# Check if there are any data points within Karnataka
if len(karnataka_data) == 0:
    print("No data points found within Karnataka. Please adjust the bounding box.")

else:
    # Perform clustering using DBSCAN
    clustering = DBSCAN(eps=0.1, min_samples=10).fit(karnataka_data[['Latitude', 'Longitude']])
    labels = clustering.labels_

    # Visualize crime hotspots based on clusters
    plt.figure(figsize=(10, 8))
    plt.scatter(karnataka_data['Longitude'], karnataka_data['Latitude'], c=labels, cmap='viridis', s=5)
    plt.title('Crime Hotspots Identified by DBSCAN')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.colorbar(label='Cluster')
    plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Load the FIR_Details_Data.csv file into a GeoDataFrame
#fir_data = gpd.read_file('FIR_Details_Data.csv')

# Convert latitude and longitude columns to numeric data types
fir_data['Latitude'] = pd.to_numeric(fir_data['Latitude'], errors='coerce')
fir_data['Longitude'] = pd.to_numeric(fir_data['Longitude'], errors='coerce')

# Filter out rows with missing or invalid latitude and longitude values
fir_data = fir_data.dropna(subset=['Latitude', 'Longitude'])

# Define the bounding box for Karnataka
karnataka_bbox = {
    'min_lat': 11.5934,
    'max_lat': 18.1667,
    'min_lon': 74.0500,
    'max_lon': 78.5885
}

# Filter out locations outside Karnataka
karnataka_data = fir_data[
    (fir_data['Latitude'] >= karnataka_bbox['min_lat']) &
    (fir_data['Latitude'] <= karnataka_bbox['max_lat']) &
    (fir_data['Longitude'] >= karnataka_bbox['min_lon']) &
    (fir_data['Longitude'] <= karnataka_bbox['max_lon'])
]

# Check if there are any data points within Karnataka
if len(karnataka_data) == 0:
    print("No data points found within Karnataka. Please adjust the bounding box.")

else:
    # Perform clustering using DBSCAN
    clustering = DBSCAN(eps=0.1, min_samples=10).fit(karnataka_data[['Latitude', 'Longitude']])
    labels = clustering.labels_

    # Visualize crime hotspots based on clusters
    plt.figure(figsize=(10, 8))
    unique_labels = set(labels)
    for cluster_label in unique_labels:
        if cluster_label == -1:  # Noise points
            cluster_points = karnataka_data[labels == cluster_label]
            plt.scatter(cluster_points['Longitude'], cluster_points['Latitude'], color='gray', s=5, label='Noise')
        else:  # Points in a cluster
            cluster_points = karnataka_data[labels == cluster_label]
            plt.scatter(cluster_points['Longitude'], cluster_points['Latitude'], s=5, label=f'Cluster {cluster_label}')

    plt.title('Crime Clusters Identified by DBSCAN')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.legend()
    plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the FIR_Details_Data.csv file into a GeoDataFrame
#fir_data = gpd.read_file('FIR_Details_Data.csv')

# Convert latitude and longitude columns to numeric data types
fir_data['Latitude'] = pd.to_numeric(fir_data['Latitude'], errors='coerce')
fir_data['Longitude'] = pd.to_numeric(fir_data['Longitude'], errors='coerce')

# Filter out rows with missing or invalid latitude and longitude values
fir_data = fir_data.dropna(subset=['Latitude', 'Longitude'])

# Define the bounding box for Karnataka
karnataka_bbox = {
    'min_lat': 11.5934,
    'max_lat': 18.1667,
    'min_lon': 74.0500,
    'max_lon': 78.5885
}

# Filter out locations outside Karnataka
karnataka_data = fir_data[
    (fir_data['Latitude'] >= karnataka_bbox['min_lat']) &
    (fir_data['Latitude'] <= karnataka_bbox['max_lat']) &
    (fir_data['Longitude'] >= karnataka_bbox['min_lon']) &
    (fir_data['Longitude'] <= karnataka_bbox['max_lon'])
]

# Check if there are any data points within Karnataka
if len(karnataka_data) == 0:
    print("No data points found within Karnataka. Please adjust the bounding box.")

else:
    # Perform K-means clustering
    kmeans = KMeans(n_clusters=5, random_state=0).fit(karnataka_data[['Latitude', 'Longitude']])
    karnataka_data['Cluster'] = kmeans.labels_

    # Visualize crime hotspots based on clusters
    plt.figure(figsize=(10, 8))
    plt.scatter(karnataka_data['Longitude'], karnataka_data['Latitude'], c=karnataka_data['Cluster'], cmap='viridis', s=5)
    plt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 0], c='red', marker='x', s=100, label='Cluster Centers')
    plt.title('Crime Clusters Identified by K-means')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.colorbar(label='Cluster')
    plt.legend()
    plt.show()

import pandas as pd
from sklearn.cluster import DBSCAN

# Load the FIR_Details_Data.csv file into a DataFrame
fir_data = pd.read_csv('selected_fir_data.csv')

# Define parameters for DBSCAN
eps = 0.1  # Distance threshold for clustering
min_samples = 10  # Minimum number of samples required to form a cluster

# Get unique districts, crime groups, and crime heads
unique_districts = fir_data['District_Name'].unique()
unique_crime_groups = fir_data['CrimeGroup_Name'].unique()
unique_crime_heads = fir_data['CrimeHead_Name'].unique()

# Function to perform DBSCAN clustering
def perform_dbscan(data, eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    cluster_labels = dbscan.fit_predict(data[['Latitude', 'Longitude']])
    return cluster_labels

# Iterate over each unique district, crime group, and crime head
for district in unique_districts:
    for crime_group in unique_crime_groups:
        for crime_head in unique_crime_heads:
            # Filter data for the current combination of district, crime group, and crime head
            filtered_data = fir_data[(fir_data['District_Name'] == district) &
                                     (fir_data['CrimeGroup_Name'] == crime_group) &
                                     (fir_data['CrimeHead_Name'] == crime_head)]

            # Check if there is data for the current combination
            if not filtered_data.empty:
                # Perform DBSCAN clustering
                cluster_labels = perform_dbscan(filtered_data, eps, min_samples)

                # Assign cluster labels to the filtered data
                filtered_data['Cluster_Labels'] = cluster_labels

                # Now you can do further processing or visualization with the clustered data
                # For example, you can save the clustered data to a CSV file

                # Save the clustered data to a CSV file
                filename = f"{district}_{crime_group}_{crime_head}_clusters.csv"
                filtered_data.to_csv(filename, index=False)
                print(f"DBSCAN clustering completed for {district}, {crime_group}, {crime_head}. Clustered data saved to {filename}.")
            else:
                print(f"No data found for {district}, {crime_group}, {crime_head}. Skipping DBSCAN clustering.")

import pandas as pd
from sklearn.cluster import DBSCAN
import os

# Load the FIR_Details_Data.csv file into a DataFrame
#fir_data = pd.read_csv('FIR_Details_Data.csv')

# Define parameters for DBSCAN
eps = 0.1  # Distance threshold for clustering
min_samples = 10  # Minimum number of samples required to form a cluster

# Get unique districts, crime groups, and crime heads
unique_districts = fir_data['District_Name'].unique()
unique_crime_groups = fir_data['CrimeGroup_Name'].unique()
unique_crime_heads = fir_data['CrimeHead_Name'].unique()

# Function to perform DBSCAN clustering
def perform_dbscan(data, eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    cluster_labels = dbscan.fit_predict(data[['Latitude', 'Longitude']])
    return cluster_labels

# Iterate over each unique district, crime group, and crime head
for district in unique_districts:
    for crime_group in unique_crime_groups:
        for crime_head in unique_crime_heads:
            # Filter data for the current combination of district, crime group, and crime head
            filtered_data = fir_data[(fir_data['District_Name'] == district) &
                                     (fir_data['CrimeGroup_Name'] == crime_group) &
                                     (fir_data['CrimeHead_Name'] == crime_head)]

            # Check if there is data for the current combination
            if not filtered_data.empty:
                # Perform DBSCAN clustering
                cluster_labels = perform_dbscan(filtered_data, eps, min_samples)

                # Assign cluster labels to the filtered data
                filtered_data['Cluster_Labels'] = cluster_labels

                # Modify the district, crime group, and crime head names to create a valid filename
                district = district.replace('/', '-')  # Replace '/' with '-'
                crime_group = crime_group.replace('/', '-')  # Replace '/' with '-'
                crime_head = crime_head.replace('/', '-')  # Replace '/' with '-'

                # Create a directory to save the clustered data if it does not exist
                directory = f"clustered_data/{district}/{crime_group}/{crime_head}"
                os.makedirs(directory, exist_ok=True)

                # Save the clustered data to a CSV file
                filename = f"{directory}/clusters.csv"
                filtered_data.to_csv(filename, index=False)
                print(f"DBSCAN clustering completed for {district}, {crime_group}, {crime_head}. Clustered data saved to {filename}.")
            else:
                print(f"No data found for {district}, {crime_group}, {crime_head}. Skipping DBSCAN clustering.")

import pandas as pd
import folium
from folium.plugins import MarkerCluster
from sklearn.cluster import DBSCAN
import os

# Load the FIR_Details_Data.csv file into a DataFrame
#fir_data = pd.read_csv('FIR_Details_Data.csv')

# Define parameters for DBSCAN
eps = 0.1  # Distance threshold for clustering
min_samples = 10  # Minimum number of samples required to form a cluster

# Get unique districts and crime groups
unique_districts = fir_data['District_Name'].unique()
unique_crime_groups = fir_data['CrimeGroup_Name'].unique()

# Function to perform DBSCAN clustering
def perform_dbscan(data, eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    cluster_labels = dbscan.fit_predict(data[['Latitude', 'Longitude']])
    return cluster_labels

# Iterate over each unique district and crime group
for district in unique_districts:
    for crime_group in unique_crime_groups:
        # Filter data for the current combination of district and crime group
        filtered_data = fir_data[(fir_data['District_Name'] == district) &
                                 (fir_data['CrimeGroup_Name'] == crime_group)]

        # Check if there is data for the current combination
        if not filtered_data.empty:
            # Perform DBSCAN clustering
            cluster_labels = perform_dbscan(filtered_data, eps, min_samples)

            # Assign cluster labels to the filtered data
            filtered_data['Cluster_Labels'] = cluster_labels

            # Create a Folium map centered around the mean latitude and longitude of the data
            map_center = [filtered_data['Latitude'].mean(), filtered_data['Longitude'].mean()]
            my_map = folium.Map(location=map_center, zoom_start=10)

            # Create a MarkerCluster layer for crime locations
            marker_cluster = MarkerCluster(name='Crime Locations', overlay=True, control=False)

            # Add markers for each crime location to the MarkerCluster
            for index, row in filtered_data.iterrows():
                popup_text = f"Crime Group: {row['CrimeGroup_Name']}<br>Crime Head: {row['CrimeHead_Name']}<br>Latitude: {row['Latitude']}<br>Longitude: {row['Longitude']}"
                folium.Marker(location=[row['Latitude'], row['Longitude']], popup=popup_text).add_to(marker_cluster)

            # Add the MarkerCluster to the map
            my_map.add_child(marker_cluster)

            # Add layer control
            folium.LayerControl().add_to(my_map)

            # Save the map to an HTML file
            directory = f"clustered_maps/{district}/{crime_group}"
            os.makedirs(directory, exist_ok=True)
            filename = f"{directory}/map.html"
            my_map.save(filename)
            print(f"Map created for {district}, {crime_group}. Map saved to {filename}.")
        else:
            print(f"No data found for {district}, {crime_group}. Skipping map creation.")

